---
title: "Assignment 3: Bayesian Analysis  \nLighthouse Problem\n"
author: "Shane Weisz (WSZSHA001)"
date: "19/09/2019"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{float} \makeatletter\renewcommand*{\fps@figure}{H}\makeatother
---

This assignment involves the use of Bayesian Analysis methods to solve 'The Lighthouse Problem', where we attempt to draw inference on the coordinates of a lighthouse at an unknown location in the sea given a series of flashes emitted by the lighthouse that are observed at different points on the coastline.

# Questions
## (i) Derivation of posterior distribution of $\alpha,\beta| \pmb x$

We want to derive the posterior distribution of $\alpha,\beta|\boldsymbol{x}$, which can be expressed as:
$$\pi(\alpha,\beta|\boldsymbol{x}) = \frac{\pi(\alpha,\beta) \text{L}(\alpha,\beta;\boldsymbol{x})}{m(\boldsymbol{x})}= \frac{g(\alpha,\beta;\boldsymbol{x})}{m(\boldsymbol{x})},$$ where 
$g(\alpha,\beta;\boldsymbol{x})=\pi(\alpha,\beta) \text{L}(\alpha,\beta;\boldsymbol{x})$.

We proceed by calculating each of these components individually:

#### Prior: $\pi(\alpha,\beta)$

From the context of the problem, we can make the assumption that $\alpha$ and $\beta$ are independent. As such, we can obtain our prior distribution from $$\pi(\alpha,\beta)=\pi(\alpha)\pi(\beta)$$. 

We are given that $\alpha \sim \text{U}(0,5)$ and $\beta \sim \text{Gamma}(4,8)$, and so substituting the appropriate density functions yields our **prior distribution**: $$\pi(\alpha,\beta)=\pi(\alpha)\pi(\beta) = \frac{1}{5}\frac{8^4 \beta^3 e^{-8\beta}}{\Gamma(4)}=\frac{2048\beta^3 e^{-8\beta}}{15}$$

#### Likelihood: $\text{L}(\alpha,\beta$;$\boldsymbol{x}$)

We first note that we calculate the likelihood as follows:
$$\text{L}(\alpha,\beta, \boldsymbol{x})=\prod_{i=1}^{200} f(x_i|\alpha,\beta)$$

We thus need to calculate the density function for the data, $f(x_i|\alpha,\beta)$. 
So let $\theta_i$ be the angle of inclination of the line joining the lighthouse to the point on the coastline where the $i^{th}$ emission was recorded. It follows that $\tan(\theta_i)=\frac{x_i-\alpha}{\beta}$ and hence that $\theta_l=-\arctan(\frac{\alpha}{\beta})<\theta_k<\arctan(\frac{5-\alpha}{\beta})=\theta_u$.

Now the problem definition allows us to make the assumption that $\theta_i \sim\text{U}(\theta_l,\theta_u)$, which yields the p.d.f. of $\theta_i$ as $\pi(\theta_i)=\frac{1}{\theta_u-\theta_l}=\frac{1}{\text{A}(\alpha,\beta)}$, where we define $\text{A}(\alpha,\beta) = \theta_u-\theta_l$.

Applying an appropriate univariate transformation, we obtain that $f(x_i|\alpha,\beta)=\frac{1}{\text{A}(\alpha,\beta)}\begin{vmatrix}\frac{d \theta_i}{d {x_i}}\end{vmatrix}$ where $\frac{d \theta_i}{d {x_i}}=\frac{d}{d {x_i}}\arctan(\frac{x_i-\alpha}{\beta})=\frac{\beta}{\beta^2+(x_k-\alpha)^2}.$
Since $\beta>0$, we get that $\frac{\beta}{\beta^2+(x_k-\alpha)^2}>0$, and so  $f(x_i|\alpha,\beta)=\frac{1}{\text{A}(\alpha,\beta)}\cdot\frac{\beta}{\beta^2+(x_i-\alpha)^2}$.

Substituting into the above formula for the likelihood gives us our **likelihood** function:
$$\text{L}(\alpha,\beta, \boldsymbol{x})=\prod_{i=1}^{200} f(x_i|\alpha,\beta) = \prod_{i=1}^{200} \frac{1}{\text{A}(\alpha,\beta)}\cdot\frac{\beta}{\beta^2+(x_i-\alpha)^2}$$

#### Predictive pdf of x: $m(\boldsymbol{x})$

By the Law of Total Probability, we have that $$m(\boldsymbol{x})=\int_{0}^5 \int_{0}^{\infty} \pi(\alpha,\beta) \text{L}(\alpha,\beta;\boldsymbol{x}) d\beta d\alpha$$ where $\pi(\alpha,\beta)=\frac{2048\beta^3 e^{-8\beta}}{15}$ and $\text{L}(\alpha,\beta, \boldsymbol{x})= \prod_{i=1}^{200} \frac{1}{\text{A}(\alpha,\beta)}\cdot\frac{\beta}{\beta^2+(x_i-\alpha)^2}$, as derived above.

#### Resulting posterior distribution of $\alpha,\beta| \pmb x$

Combining the above expressions, we thus get that $$\pi(\alpha,\beta|\boldsymbol{x}) = \frac{g(\alpha,\beta;\boldsymbol{x})}{m(\boldsymbol{x})},$$ where \label{sec:g}
$$ g(\alpha,\beta;\boldsymbol{x})=\pi(\alpha,\beta) \text{L}(\alpha,\beta;\boldsymbol{x}),$$
$$\pi(\alpha,\beta)=\frac{2048\beta^3 e^{-8\beta}}{15},$$
,$$\text{L}(\alpha,\beta, \boldsymbol{x})= \prod_{i=1}^{200} \frac{1}{\text{A}(\alpha,\beta)}\cdot\frac{\beta}{\beta^2+(x_i-\alpha)^2},$$
$$\text{A}(\alpha,\beta) = \arctan(\frac{5-\alpha}{\beta}) + \arctan(\frac{\alpha}{\beta}),$$
and finally $$m(\boldsymbol{x})=\int_{0}^5 \int_{0}^{\infty} \pi(\alpha,\beta) \text{L}(\alpha,\beta;\boldsymbol{x}) d\beta d\alpha.$$

## (ii) Function to calculate g($\alpha$,$\beta$;x) and obtain contour plot

```{r loadData, include=FALSE}
load("~/Documents/UCT/3rd Year/STA3043S/Assignments/Assignment3/Assignment3Data2019.RData")
x_data = Data['wszsha001',]
```

The R function `gfunc` (see \hyperref[sec:appendix]{\textbf{Appendix}}) was written to calculate g($\alpha,\beta; \pmb x$), using the expression derived in  \hyperref[sec:g]{\textbf{Question (i)}} above.

```{r gfunc, echo=FALSE}

# ================== Question (ii) =====================

# A(alpha, beta) is defined as the difference between the upper and lower bounds for theta
A = function(alpha, beta)
{
   return( (atan((5 - alpha) / beta)) - (-atan(alpha / beta)) )
}

# Returns the likelihood function for alpha and beta given an observed data vector x
L = function(alpha, beta, x)
{
   return( prod( (1 / A(alpha, beta)) * beta / (beta ^ 2 + (x - alpha)^2)) )
}

# Returns g(alpha,beta;x) - the numerator of the posterior
gfunc = function(alpha, beta, x)
{
    prior_alpha = dunif(alpha, min = 0, max = 5)
    prior_beta  = dgamma(beta, shape = 4, rate = 8)
    return( prior_alpha * prior_beta * L(alpha, beta, x) )
}
gfunc.vec = Vectorize(gfunc, vectorize.args = c("alpha","beta"))

```

We then create a sequence of $\alpha$ values ranging between 0 and 5, and a sequence of $\beta$ values between 0 and 1.5. The function described above is then evaluated at each $\alpha$ and $\beta$ combination (i.e. we evaluate g($\alpha,\beta; \pmb x$) at each combination) to obtain the contour plot proportional to $\pi(\alpha,\beta| \pmb x)$ below.

```{r contourPlot, echo=FALSE, fig.cap="Contour Plot", fig.width = 4.5, fig.height = 4.5, fig.align = 'center'}
n = 100
alpha_range = seq(from = 0, to = 5, length.out = n)
beta_range  = seq(from = 0, to = 1.5, length.out = n)
g = outer(alpha_range, beta_range, FUN = gfunc.vec, x_data)

contour(x = alpha_range, 
        y = beta_range, 
        z = g, 
        xlab=expression(alpha),
        ylab=expression(beta),
        main=expression(paste("Proportional to ", pi, "(", alpha, ",", beta, "|x)")), 
        xlim=range(alpha_range), 
        ylim=range(beta_range), 
        col="blue",
        nlevels = 10)
```

Notice how highly concentrated the plot is around the point $\alpha \approx 2.6$ and $\beta \approx 0.8$. This corresponds to a significant 'spike' in the three dimensional surface around this point. We will continue our discussion of this plot in \hyperref[sec:iii]{\textbf{Question (iii)}} below.

```{r eval=FALSE, include=FALSE}
library(plotly)
plot_ly(x = alpha_range, 
        y = beta_range, 
        z = g) %>% add_surface()
```

## (iii) Values of $\alpha$, $\beta$ that maximises $\pi$($\alpha$,$\beta$|x)

\label{sec:iii}

We now want to obtain the values of $\alpha$, $\beta$ that maximises $\pi$($\alpha$,$\beta | \pmb x$). Note that $m(\pmb x)$ is a constant, and that the log function is a mononotonically increasing function. Thus the values that maximise $\pi$($\alpha$,$\beta | \pmb x$) are the same as the values that maximise $\text{log}(g(\alpha,\beta; \pmb x))$.

```{r maximise, echo=FALSE}

# ================== Question (iii) =====================

# A function that returns a value proportional to the negative log of the posterior
negLogPost = function(params, x)
{
    alpha = params[1]
    beta  = params[2]
    prior_alpha = dunif(alpha, min = 0, max = 5)
    prior_beta  = dgamma(beta, shape = 4, rate = 8)
    return( - log( prior_alpha * prior_beta * L(alpha, beta, x) ) )
}

estimates = optim( par = c(2.5, 0.5), 
                   fn = negLogPost, 
                   x = x_data,
                   lower = c(0.0001, 0.0001),
                   upper = c(5, Inf),
                   method = "L-BFGS-B")$par
names(estimates) = c('alpha', 'beta')
```

We then use a numerical optimization technique in R to maximise $\text{log}(g(\alpha,\beta; \pmb x))$ and obtain the resulting maximising values of $\alpha$ and $\beta$ below.
 
```{r tableForEstimates, echo=FALSE}
library(knitr)
library(kableExtra)

headings = c('$\\alpha $', '$\\beta$')

estimates_mtx <- matrix(c(round(estimates, 4)), ncol = 2)
colnames(estimates_mtx) <- c(headings)

kable(estimates_mtx, format = 'latex', align="c", escape = F, 
      caption = "Values of $\\alpha$ and $\\beta$ that maximises  $\\pi$($\\alpha$,$\\beta$|x)") %>% 
  kable_styling(latex_options = c("hold_position"))  
```

Notice that these values correspond to the point inside the innermost contour in the contour plot above.

## (iv) Function to obtain marginal distribution of $\alpha|\pmb x$ using numerical integration

The R function `alpham` (see \hyperref[sec:appendix]{\textbf{Appendix}}) was written to obtain the marginal distribution of $\alpha|x$,  given by $$\pi(\alpha| \pmb x) = \frac{ \int_{ \beta} \pi(\alpha) \pi( \beta) \text{L}( \alpha, \beta, \pmb x)d \beta.}{m(\pmb x)}, $$ using the expressions derived in \hyperref[sec:g]{\textbf{Question (i)}} above.

The function uses numerical integration in R to first evaluate the integral given by $$ m(\pmb x)=\int_{0}^5 \int_{0}^{\infty} \pi(\alpha,\beta) \text{L}(\alpha,\beta;\pmb x) d\beta d\alpha, $$ and then to integrate the above joint posterior density of $\alpha$ and $\beta$ with respect to $\beta$ to obtain the density function of the marginal distribution of $\alpha|\pmb x$. This density function is then evaluated across a range of $\alpha$ values between 1.5 and 3.5 and plotted below. 

```{r alpham, echo=FALSE}

# ================== Question (iv) =====================

library(cubature)

gfunc.int = function(params, x)
{
    alpha = params[1]
    beta  = params[2]
    prior_alpha = dunif(alpha, min = 0, max = 5)
    prior_beta  = dgamma(beta, shape = 4, rate = 8)
    return( prior_alpha * prior_beta * L(alpha, beta, x) )
}

m_x = adaptIntegrate(gfunc.int, 
                     lowerLimit = c(0.0001, 0.0001), 
                     upperLimit = c(5, 1.5), 
                     x = x_data)$integral


gfunc.int_beta =  Vectorize(gfunc, vectorize.args = c("beta"))
alpham = function(alpha, x)
{
   numerator   = integrate(gfunc.int_beta, 
                           lower = 0.0001, 
                           upper = 1.5, 
                           alpha = alpha, 
                           x = x)$value
   denominator = m_x
   return( numerator / denominator  )
}
alpham.vec = Vectorize(alpham, vectorize.args = "alpha")
```

```{r plotDbn, echo=FALSE, fig.cap = "Density function of the marginal distribution of $\\alpha| \\pmb x$", , fig.width = 4.5, fig.height = 4.5, fig.align = 'center'}
n = 1000
x = seq(from = 1.5, to = 3.5, length.out = n)
y = NULL
for(i in 1:n)
{
   y[i] = alpham(x[i], x_data)
}

plot(y ~ x, 
     col  = 'red',
     lty  = 1,
     type = 'l',
     xlab = expression(alpha), 
     ylab = "",
     main = expression(paste(pi, "(", alpha, "|x)")))
```

## (v) Estimates of the posterior mean and variance of $\alpha$

The above-calculated density function of $\alpha|\pmb x$ can now be used to provide estimates for the posterior mean and variance of $\alpha.$

For the mean, we evaluate the integral $$\text{E[}\alpha | \pmb x \text{]} = \int_{\alpha}\alpha\cdot\pi(\alpha | \pmb x) d\alpha$$.

For the variance, we calculate $$\text{Var[}\alpha | \pmb x \text{]} = \text{E[}\alpha ^ {2} | \pmb x \text{]} - (\text{E[}\alpha | \pmb x \text{]})^{2}, $$ where
$$\text{E[}\alpha^{2} | \pmb x \text{]} = \int_{\alpha}\alpha^{2}\cdot\pi(\alpha | \pmb x) d\alpha$$
We use numerical integration in R to evaluate these integrals and obtain the posterior estimates for the mean and variance of $\alpha$ given below.

```{r estimates, echo=FALSE}

# ================== Question (v) =====================

MeanIntegrand = function(alpha, x)
{
  return( alpha * alpham(alpha, x) )
}
MeanIntegrand.vec = Vectorize(MeanIntegrand, vectorize.args = c("alpha"))
mean = integrate(MeanIntegrand.vec, lower = 2, upper = 3, x = x_data)$value

VarIntegrand = function(alpha, x)
{
  return( alpha^2 * alpham(alpha, x) )
}
VarIntegrand.vec = Vectorize(VarIntegrand, vectorize.args = c("alpha"))
x_sq = integrate(VarIntegrand.vec, lower = 2, upper = 3, x = x_data)$value
variance =  x_sq - mean^2

headings = c('Mean', 'Variance') 
estimates_mtx <- matrix(c(round(mean, 4), round(variance, 4)), ncol = 2)
colnames(estimates_mtx) <- c(headings)

kable(estimates_mtx, format = 'latex', align="c", escape = F, 
      caption = "Estimates of the posterior mean and variance of $\\alpha$") %>% 
  kable_styling(latex_options = c("hold_position")) 
```

## (vi) Acceptance-rejection method for inference on $\alpha$

We now use the acceptance-rejection method to provide 20 000 draws from $\alpha| \pmb x$, and then use these draws to provide estimates of the posterior mean and variance of $\alpha.$ 

The method used to perform this procedure is as follows. We simulate a random (x,y) point in the rectange with corners (-2.25, 0), (2.85, 0), (-2.25, 4.6) and (2.85, 4.6). Note that this region contains the curve of the density function of $\alpha| \pmb x$ for the $\alpha$ values where the density is not negligible (i.e. non-zero). For each simulated (x, y) point, we accept the point (and store the corresponding $\alpha$ value) if it lies below the curve of $\pi(\alpha| \pmb x)$, otherwise we reject the point. We continue simulating points until 20 000 points have been accepted. The $\alpha$ values corresponding to the accepted points can be proven to have the same distribution as $\alpha| \pmb x$, and hence correspond to draws from $\alpha| \pmb x$. A sample plot of the simulated points which would be accepted during this procedure is depicted below.

```{r plotAR, echo=FALSE, fig.cap = "Accepted points from Acceptance-Rejection procedure", fig.width = 4, fig.height = 4, fig.align = 'center'}

# ================== Question (vi) =====================

# For a sample plot of the accepted simulated points
plot(y ~ x,
     col  = 'red',
     lty  = 1,
     type = 'l',
     xlab = expression(alpha),
     ylab = "",
     main = expression(paste(pi, "(", alpha, "|x)")))

set.seed(2019)
n = 1000
xx <- runif(n, min=2.25, max=2.85)
yy <- runif(n, min=0, max=4.6) # since max(y) = 4.579861 where y has the density values 

retained_ids = yy <= alpham.vec(xx, x_data)

points(xx[retained_ids], yy[retained_ids], pch = 19, cex = 0.2, col="blue")
```

The below histogram of the 20 000 draws, with the true density function of $\alpha| \pmb x$ superimposed in red, is visual evidence that these draws have the distribution of $\alpha| \pmb x$.

```{r arHist, echo=FALSE, fig.cap = "Histogram of draws obtained from $\\alpha|x$ via Acceptance-Rejection method"}

# Provides posterior draws (of at least 20 000) from alpha|x
ar = function(num_to_accept = 20000)
{
  count = 0
  draws = NULL
  while(count < num_to_accept)
  {
    x <- runif(1, min=2.25, max=2.85)
    y <- runif(1, min=0, max=4.6)
    if(y < alpham(x, x_data))
    {
       draws[count] = x
       count = count + 1
    }
  }
  return(draws)
}

draws = ar()

hist(draws, 
     freq = F, 
     col = 'grey',
     xlab = expression(alpha),
     main = expression(paste("Histogram of 20 000 draws from ", alpha, "|x")))

lines(y ~ x,
     col  = 'red',
     lty  = 1,
     type = 'l')

legend('topright',
       legend = c(expression(paste("Draws from ", alpha, "|x")),
                  expression(paste("True density of ", alpha, "|x"))),
       fill = c('grey','red'))

```


Using these draws, we can then calculate the sample mean and sample variance as another means of providing a posterior estimate for the mean and variance of $\alpha$. These estimates are tabulated below:

```{r ar, echo=FALSE}

# Tabulate the results
headings = c('Mean', 'Variance') 
estimates_mtx <- matrix(c(round(mean(draws), 4), round(var(draws), 4)), ncol = 2)
colnames(estimates_mtx) <- c(headings)
kable(estimates_mtx, format = 'latex', align="c", escape = F, 
      caption = "Estimates of the posterior mean and variance of $\\alpha$ using Acceptance-Rejection") %>% 
  kable_styling(latex_options = c("hold_position")) 
```

We now compare these estimates to those obtained in (v), and notice that they are very simlar. The posterior draws produce a mean of **2.5587** compared to **2.5991** obtained in (v) via integration, and a similarly close estimate for variances of **0.0075** compared to **0.0076** from integration. It makes sense that these estimates are consistent, because whilst the methods are different, they are still estimating the same parameter and so would be expected to produce similar estimates if performed correctly.

\newpage
# Appendix
\label{sec:appendix}

```{r ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}

```